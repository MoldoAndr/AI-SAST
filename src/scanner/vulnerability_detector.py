"""
Vulnerability detector module for AI_SAST.

Uses OpenAI to detect vulnerabilities in code files.
"""

import os
import json
import logging
import time
from pathlib import Path
from typing import Dict, List, Any, Tuple
from concurrent.futures import ThreadPoolExecutor
from rich.progress import Progress
import jsonschema

from .openai_client import get_openai_client, call_openai_with_retry
from .config import Config

logger = logging.getLogger("ai_sast")

VULNERABILITY_SCHEMA = {
    "type": "object",
    "properties": {
        "vulnerability_type": {"type": "string"},
        "description": {"type": "string"},
        "location": {
            "type": "object",
            "properties": {
                "file": {"type": "string"},
                "line": {"type": ["integer", "string"]},
                "column": {"type": ["integer", "string", "null"]}
            },
            "required": ["file", "line"]
        },
        "severity": {"type": "string"},
        "recommendation": {"type": "string"}
    },
    "required": ["vulnerability_type", "description", "location", "severity", "recommendation"]
}

EXAMPLE_VULNERABILITIES = [
    {
        "vulnerability_type": "Cross-Site Scripting (XSS)",
        "description": "Unsanitized user input is directly inserted into the HTML, allowing attackers to inject malicious scripts.",
        "location": {
            "file": "./components/UserProfile.jsx",
            "line": 42,
            "column": 15
        },
        "severity": "high",
        "recommendation": "Use React's built-in XSS protection by using curly braces for dynamic content. For HTML attributes, consider using a sanitization library like DOMPurify."
    },
    {
        "vulnerability_type": "Insecure Authentication",
        "description": "Credentials are stored in local storage which is vulnerable to XSS attacks.",
        "location": {
            "file": "./services/auth.js",
            "line": 78,
            "column": 3
        },
        "severity": "critical",
        "recommendation": "Use HttpOnly cookies for sensitive authentication tokens instead of local storage. Consider using a secure authentication library or service."
    }
]


def get_vulnerability_prompt(file_path: Path, file_content: str, related_files: List[Path] = None) -> str:
    """
    Construct a prompt for vulnerability detection.
    
    Args:
        file_path: Path to the file
        file_content: Content of the file
        related_files: List of related files
        
    Returns:
        str: Prompt for OpenAI
    """
    related_files_info = ""
    if related_files and len(related_files) > 0:
        related_files_info = "This file has relationships with the following files:\n"
        for rf in related_files[:5]:
            related_files_info += f"- {rf.name}\n"
        if len(related_files) > 5:
            related_files_info += f"And {len(related_files) - 5} more files\n"
    
    prompt = {
        "ai_role": "cyber_security_expert",
        "request": "analyze this file for security vulnerabilities using static analysis",
        "frontend_vulnerabilities": [
            "Cross-Site Scripting (XSS)",
            "Cross-Site Request Forgery (CSRF)",
            "Insecure Authentication",
            "Insecure Data Storage",
            "Insecure API Endpoints",
            "Information Leakage",
            "Improper Access Control",
            "Insecure Dependencies",
            "Prototype Pollution",
            "Path Traversal",
            "Server-Side Request Forgery (SSRF)",
            "SQL Injection",
            "NoSQL Injection",
            "DOM-based vulnerabilities",
            "Sensitive Data Exposure"
        ],
        "structure_of_vuln": {
            "vulnerability_type": "string - one of the vulnerability types listed above",
            "description": "string - detailed description of the vulnerability",
            "location": {
                "file": "string - file path relative to project root",
                "line": "integer - line number where vulnerability exists",
                "column": "integer - column number where vulnerability starts"
            },
            "severity": "string - must be one of: critical, high, medium, low",
            "recommendation": "string - specific remediation advice"
        },
        "example_vulnerabilities": EXAMPLE_VULNERABILITIES,
        "file_path": str(file_path),
        "related_files_info": related_files_info,
        "content_of_file": file_content
    }
    
    return json.dumps(prompt, indent=2)


def extract_vulnerabilities(response_text: str) -> List[Dict[str, Any]]:
    """
    Extract and validate vulnerabilities from OpenAI response.
    
    Args:
        response_text: Text response from OpenAI
        
    Returns:
        List[Dict[str, Any]]: List of validated vulnerabilities
    """
    vulnerabilities = []
    
    try:
        data = json.loads(response_text)
        
        if isinstance(data, list):
            vulnerabilities = data
        elif isinstance(data, dict) and "vulnerability_type" in data:
            vulnerabilities = [data]
        elif isinstance(data, dict) and "vulnerabilities" in data:
            vulnerabilities = data["vulnerabilities"]
    except json.JSONDecodeError:
        import re
        json_pattern = r'\{\s*"vulnerability_type"\s*:.*?\}\s*(?=\{|$)'
        matches = re.finditer(json_pattern, response_text, re.DOTALL)
        
        for match in matches:
            try:
                vuln = json.loads(match.group(0))
                vulnerabilities.append(vuln)
            except json.JSONDecodeError:
                continue
    
    validated_vulns = []
    for vuln in vulnerabilities:
        try:
            jsonschema.validate(instance=vuln, schema=VULNERABILITY_SCHEMA)
            validated_vulns.append(vuln)
        except jsonschema.exceptions.ValidationError as e:
            logger.warning(f"Invalid vulnerability format: {str(e)}")
    
    return validated_vulns


def detect_vulnerabilities(file_path: Path, file_content: str, related_files: List[Path], config: Config) -> List[Dict[str, Any]]:
    """
    Detect vulnerabilities in a file using OpenAI.
    
    Args:
        file_path: Path to the file
        file_content: Content of the file
        related_files: List of related files
        config: Configuration object
        
    Returns:
        List[Dict[str, Any]]: List of detected vulnerabilities
    """
    try:
        prompt = get_vulnerability_prompt(file_path, file_content, related_files)
        
        client = get_openai_client()
        
        response = call_openai_with_retry(
            client=client,
            model=config.model,
            messages=[
                {"role": "system", "content": "You are a cybersecurity expert specializing in finding security vulnerabilities in code. Respond with JSON only."},
                {"role": "user", "content": prompt}
            ],
            temperature=config.temperature,
            max_tokens=config.max_tokens,
            max_retries=config.max_retries,
            retry_delay=config.retry_delay
        )
        
        response_text = response.choices[0].message.content
        vulnerabilities = extract_vulnerabilities(response_text)
        
        for vuln in vulnerabilities:
            vuln["location"]["file"] = str(file_path)
        
        return vulnerabilities
    
    except Exception as e:
        logger.error(f"Error detecting vulnerabilities in {file_path}: {str(e)}")
        return []


def write_vulnerability_report(vulnerabilities: List[Dict[str, Any]], output_dir: Path) -> None:
    """
    Write vulnerability report to JSON files.
    
    Args:
        vulnerabilities: List of vulnerabilities
        output_dir: Output directory
    """
    vuln_by_type = {}
    for vuln in vulnerabilities:
        vuln_type = vuln["vulnerability_type"]
        sanitized_name = vuln_type.lower().replace(" ", "_").replace("(", "").replace(")", "").replace("-", "_")
        
        if sanitized_name not in vuln_by_type:
            vuln_by_type[sanitized_name] = []
        
        vuln_by_type[sanitized_name].append(vuln)
    
    for vuln_type, vulns in vuln_by_type.items():
        file_path = output_dir / f"{vuln_type}.json"
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(vulns, f, indent=2)


def scan_files_for_vulnerabilities(
    files: List[Path], 
    file_relationships: Dict[Path, List[Path]], 
    config: Config, 
    progress: Progress, 
    task_id
) -> Dict[Path, List[Dict[str, Any]]]:
    """
    Scan files for vulnerabilities.
    
    Args:
        files: List of files to scan
        file_relationships: Dictionary mapping files to their dependencies
        config: Configuration object
        progress: Progress object for updating the progress bar
        task_id: Task ID for the progress bar
        
    Returns:
        Dict[Path, List[Dict[str, Any]]]: Dictionary mapping files to their vulnerabilities
    """
    all_vulnerabilities = {}
    file_count = len(files)
    
    batch_size = min(config.batch_size, 10)
    
    file_batches = [files[i:i+batch_size] for i in range(0, len(files), batch_size)]
    
    completed = 0
    
    for batch in file_batches:
        batch_results = {}
        
        with ThreadPoolExecutor(max_workers=min(batch_size, 5)) as executor:
            future_to_file = {}
            
            for file in batch:
                try:
                    with open(file, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                    
                    related_files = file_relationships.get(file, [])
                    
                    future = executor.submit(
                        detect_vulnerabilities, 
                        file, 
                        content, 
                        related_files, 
                        config
                    )
                    future_to_file[future] = file
                except Exception as e:
                    logger.error(f"Error reading {file}: {str(e)}")
                    batch_results[file] = []
            
            for future in future_to_file:
                file = future_to_file[future]
                try:
                    vulnerabilities = future.result()
                    batch_results[file] = vulnerabilities
                    
                    completed += 1
                    progress.update(task_id, description=f"[cyan]Scanning files ({completed}/{file_count})...", completed=completed)
                    
                except Exception as e:
                    logger.error(f"Error processing {file}: {str(e)}")
                    batch_results[file] = []
        
        all_vulnerabilities.update(batch_results)
        
        if len(file_batches) > 1:
            time.sleep(1)
    
    logs_folder_name = config.get_logs_folder_name()
    output_subdir = Path(config.output_dir) / logs_folder_name
    output_subdir.mkdir(exist_ok=True, parents=True)
    
    all_vulns_list = []
    for file_vulns in all_vulnerabilities.values():
        all_vulns_list.extend(file_vulns)
    
    if all_vulns_list:
        write_vulnerability_report(all_vulns_list, output_subdir)
    
    return all_vulnerabilities