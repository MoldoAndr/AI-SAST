"""
Vulnerability detector module for AI_SAST.

Uses OpenAI to detect vulnerabilities in code files with token tracking.
"""

import os
import json
import logging
import time
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor
from rich.progress import Progress
import jsonschema

from .openai_client import get_openai_client, call_openai_with_retry
from .config import Config
from .pricing_tracker import PricingTracker

logger = logging.getLogger("ai_sast")

VULNERABILITY_SCHEMA = {
    "type": "object",
    "properties": {
        "vulnerability_type": {"type": "string"},
        "description": {"type": "string"},
        "location": {
            "type": "object",
            "properties": {
                "file": {"type": "string"},
                "line": {"type": ["integer", "string"]},
                "column": {"type": ["integer", "string", "null"]}
            },
            "required": ["file", "line"]
        },
        "severity": {"type": "string"},
        "recommendation": {"type": "string"}
    },
    "required": ["vulnerability_type", "description", "location", "severity", "recommendation"]
}

EXAMPLE_VULNERABILITIES = [
    {
        "vulnerability_type": "Cross-Site Scripting (XSS)",
        "description": "Unsanitized user input is directly inserted into the HTML, allowing attackers to inject malicious scripts.",
        "location": {
            "file": "./components/UserProfile.jsx",
            "line": 42,
            "column": 15
        },
        "severity": "high",
        "recommendation": "Use React's built-in XSS protection by using curly braces for dynamic content. For HTML attributes, consider using a sanitization library like DOMPurify."
    },
    {
        "vulnerability_type": "Insecure Authentication",
        "description": "Credentials are stored in local storage which is vulnerable to XSS attacks.",
        "location": {
            "file": "./services/auth.js",
            "line": 78,
            "column": 3
        },
        "severity": "critical",
        "recommendation": "Use HttpOnly cookies for sensitive authentication tokens instead of local storage. Consider using a secure authentication library or service."
    }
]


def get_vulnerability_prompt(file_path: Path, file_content: str, related_files: List[Path] = None) -> str:
    """
    Construct a prompt for vulnerability detection.
    
    Args:
        file_path: Path to the file
        file_content: Content of the file
        related_files: List of related files
        
    Returns:
        str: Prompt for OpenAI
    """
    related_files_info = ""
    if related_files and len(related_files) > 0:
        related_files_info = "This file has relationships with the following files:\n"
        for rf in related_files[:5]:
            related_files_info += f"- {rf.name}\n"
        if len(related_files) > 5:
            related_files_info += f"And {len(related_files) - 5} more files\n"
    
    prompt = {
        "ai_role": "cyber_security_expert",
        "request": "analyze this file for security vulnerabilities using static analysis",
        "frontend_vulnerabilities": [
            "Cross-Site Scripting (XSS)",
            "Cross-Site Request Forgery (CSRF)",
            "Insecure Authentication",
            "Insecure Data Storage",
            "Insecure API Endpoints",
            "Information Leakage",
            "Improper Access Control",
            "Insecure Dependencies",
            "Prototype Pollution",
            "Path Traversal",
            "Server-Side Request Forgery (SSRF)",
            "SQL Injection",
            "NoSQL Injection",
            "DOM-based vulnerabilities",
            "Sensitive Data Exposure"
        ],
        "structure_of_vuln": {
            "vulnerability_type": "string - one of the vulnerability types listed above",
            "description": "string - detailed description of the vulnerability",
            "location": {
                "file": "string - file path relative to project root",
                "line": "integer - line number where vulnerability exists",
                "column": "integer - column number where vulnerability starts"
            },
            "severity": "string - must be one of: critical, high, medium, low",
            "recommendation": "string - specific remediation advice"
        },
        "example_vulnerabilities": EXAMPLE_VULNERABILITIES,
        "file_path": str(file_path),
        "related_files_info": related_files_info,
        "content_of_file": file_content
    }
    
    return json.dumps(prompt, indent=2)


def extract_vulnerabilities(response_text: str) -> List[Dict[str, Any]]:
    """
    Extract and validate vulnerabilities from OpenAI response.
    
    Args:
        response_text: Text response from OpenAI
        
    Returns:
        List[Dict[str, Any]]: List of validated vulnerabilities
    """
    vulnerabilities = []
    
    try:
        data = json.loads(response_text)
        
        if isinstance(data, list):
            vulnerabilities = data
        elif isinstance(data, dict) and "vulnerability_type" in data:
            vulnerabilities = [data]
        elif isinstance(data, dict) and "vulnerabilities" in data:
            vulnerabilities = data["vulnerabilities"]
    except json.JSONDecodeError:
        import re
        json_pattern = r'\{\s*"vulnerability_type"\s*:.*?\}\s*(?=\{|$)'
        matches = re.finditer(json_pattern, response_text, re.DOTALL)
        
        for match in matches:
            try:
                vuln = json.loads(match.group(0))
                vulnerabilities.append(vuln)
            except json.JSONDecodeError:
                continue
    
    validated_vulns = []
    for vuln in vulnerabilities:
        try:
            jsonschema.validate(instance=vuln, schema=VULNERABILITY_SCHEMA)
            validated_vulns.append(vuln)
        except jsonschema.exceptions.ValidationError as e:
            logger.warning(f"Invalid vulnerability format: {str(e)}")
    
    return validated_vulns


def detect_vulnerabilities(file_path: Path, file_content: str, related_files: List[Path], 
                          config: Config, project_name: str, pricing_tracker: Optional[PricingTracker] = None) -> List[Dict[str, Any]]:
    """
    Detect vulnerabilities in a file using OpenAI.
    
    Args:
        file_path: Path to the file
        file_content: Content of the file
        related_files: List of related files
        config: Configuration object
        project_name: Name of the project (for token tracking)
        pricing_tracker: Tracker for token usage and pricing
        
    Returns:
        List[Dict[str, Any]]: List of detected vulnerabilities
    """
    try:
        prompt = get_vulnerability_prompt(file_path, file_content, related_files)
        
        client = get_openai_client()
        logger.info(f"Scanning file for vulnerabilities: {file_path}")
        
        response = call_openai_with_retry(
            client=client,
            model=config.model,
            messages=[
                {"role": "system", "content": "You are a cybersecurity expert specializing in finding security vulnerabilities in code. Respond with JSON only."},
                {"role": "user", "content": prompt}
            ],
            temperature=config.temperature,
            max_tokens=config.max_tokens,
            max_retries=config.max_retries,
            retry_delay=config.retry_delay
        )
        
        # Track token usage
        if pricing_tracker and hasattr(response, 'usage'):
            input_tokens = response.usage.prompt_tokens
            output_tokens = response.usage.completion_tokens
            
            logger.info(f"Token usage for {file_path}: {input_tokens} input, {output_tokens} output")
            
            pricing_tracker.track_usage(
                project_name=project_name,
                input_tokens=input_tokens,
                output_tokens=output_tokens
            )
        
        response_text = response.choices[0].message.content
        vulnerabilities = extract_vulnerabilities(response_text)
        
        for vuln in vulnerabilities:
            vuln["location"]["file"] = str(file_path)
        
        logger.info(f"Found {len(vulnerabilities)} vulnerabilities in {file_path}")
        return vulnerabilities
    
    except Exception as e:
        logger.error(f"Error detecting vulnerabilities in {file_path}: {str(e)}")
        return []


def write_vulnerability_report(vulnerabilities: List[Dict[str, Any]], output_dir: Path) -> None:
    """
    Write vulnerability report to JSON files.
    
    Args:
        vulnerabilities: List of vulnerabilities
        output_dir: Output directory
    """
    vuln_by_type = {}
    for vuln in vulnerabilities:
        vuln_type = vuln["vulnerability_type"]
        sanitized_name = vuln_type.lower().replace(" ", "_").replace("(", "").replace(")", "").replace("-", "_")
        
        if sanitized_name not in vuln_by_type:
            vuln_by_type[sanitized_name] = []
        
        vuln_by_type[sanitized_name].append(vuln)
    
    for vuln_type, vulns in vuln_by_type.items():
        file_path = output_dir / f"{vuln_type}.json"
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(vulns, f, indent=2)


def scan_files_for_vulnerabilities(
    files: List[Path], 
    file_relationships: Dict[Path, List[Path]], 
    config: Config, 
    progress: Progress, 
    task_id,
    pricing_tracker: Optional[PricingTracker] = None
) -> Dict[Path, List[Dict[str, Any]]]:
    """
    Scan files for vulnerabilities.
    
    Args:
        files: List of files to scan
        file_relationships: Dictionary mapping files to their dependencies
        config: Configuration object
        progress: Progress object for updating the progress bar
        task_id: Task ID for the progress bar
        pricing_tracker: Tracker for token usage and pricing
        
    Returns:
        Dict[Path, List[Dict[str, Any]]]: Dictionary mapping files to their vulnerabilities
    """
    all_vulnerabilities = {}
    file_count = len(files)
    
    # Extract project name from the first file path
    # Assumes all files are from the same project
    project_name = files[0].parts[-3] if len(files) > 0 and len(files[0].parts) >= 3 else "unknown_project"
    
    # Run CodeQL analysis if enabled
    if config.enable_codeql:
        progress.update(task_id, description="[cyan]Running CodeQL analysis...", completed=10)
        
        try:
            # Try to import the CodeQL scanner module
            try:
                from .codeql_scanner import run_codeql_scan
                codeql_available = True
            except (ImportError, ModuleNotFoundError):
                logger.warning("CodeQL scanner module not available, skipping CodeQL analysis")
                codeql_available = False
            
            if codeql_available:
                # Create paths for CodeQL database and results
                logs_folder_name = config.get_logs_folder_name()
                output_subdir = Path(config.output_dir) / logs_folder_name
                
                # Run the analysis and get results
                src_dir = Path(config.src_dir)
                codeql_findings = run_codeql_scan(src_dir, output_subdir)
                
                # Process findings into our format
                for finding in codeql_findings:
                    # Get the file path from the finding
                    file_path_str = finding["location"]["file"]
                    
                    # Convert to a relative path if it's absolute
                    if os.path.isabs(file_path_str):
                        try:
                            # Try to make it relative to the source directory
                            rel_path = os.path.relpath(file_path_str, config.src_dir)
                            file_path = src_dir / rel_path
                        except ValueError:
                            # If that fails, use the path as is
                            file_path = Path(file_path_str)
                    else:
                        # If it's already relative, make it relative to src_dir
                        file_path = src_dir / file_path_str
                    
                    # Add to our vulnerability dictionary
                    if file_path not in all_vulnerabilities:
                        all_vulnerabilities[file_path] = []
                    
                    all_vulnerabilities[file_path].append(finding)
                
                # Write CodeQL findings to a separate file
                codeql_output_path = output_subdir / "codeql_vulnerabilities.json"
                with open(codeql_output_path, 'w', encoding='utf-8') as f:
                    json.dump(codeql_findings, f, indent=2)
                
                progress.update(task_id, description=f"[green]CodeQL found {len(codeql_findings)} issues", completed=30)
                logger.info(f"CodeQL analysis found {len(codeql_findings)} potential vulnerabilities")
        except Exception as e:
            logger.error(f"Error running CodeQL analysis: {str(e)}")
            logger.exception("CodeQL analysis failed")
    else:
        logger.info("CodeQL analysis is disabled, skipping")

    # Use a batch size parameter from config with a reasonable default
    batch_size = min(config.batch_size, 10)
    
    file_batches = [files[i:i+batch_size] for i in range(0, len(files), batch_size)]
    
    completed = 0
    
    for batch in file_batches:
        batch_results = {}
        
        with ThreadPoolExecutor(max_workers=min(batch_size, 5)) as executor:
            future_to_file = {}
            
            for file in batch:
                try:
                    with open(file, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                    
                    related_files = file_relationships.get(file, [])
                    
                    future = executor.submit(
                        detect_vulnerabilities, 
                        file, 
                        content, 
                        related_files, 
                        config,
                        project_name,
                        pricing_tracker
                    )
                    future_to_file[future] = file
                except Exception as e:
                    logger.error(f"Error reading {file}: {str(e)}")
                    batch_results[file] = []
            
            for future in future_to_file:
                file = future_to_file[future]
                try:
                    vulnerabilities = future.result()
                    batch_results[file] = vulnerabilities
                    
                    completed += 1
                    progress.update(task_id, description=f"[cyan]Scanning files ({completed}/{file_count})...", completed=30 + (completed * 70 / file_count))
                    
                except Exception as e:
                    logger.error(f"Error processing {file}: {str(e)}")
                    batch_results[file] = []
        
        all_vulnerabilities.update(batch_results)
        
        if len(file_batches) > 1:
            time.sleep(1)
    
    logs_folder_name = config.get_logs_folder_name()
    output_subdir = Path(config.output_dir) / logs_folder_name
    output_subdir.mkdir(exist_ok=True, parents=True)
    
    all_vulns_list = []
    for file_vulns in all_vulnerabilities.values():
        all_vulns_list.extend(file_vulns)
    
    if all_vulns_list:
        write_vulnerability_report(all_vulns_list, output_subdir)
    
    return all_vulnerabilities
