"""
Vulnerability detector module for AI_SAST.

Uses OpenAI to detect vulnerabilities in code files with token tracking and enhanced debugging.
Enhanced JSON parsing and response handling.
"""

import os
import json
import logging
import time
import traceback
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor
from rich.progress import Progress
import jsonschema

from .openai_client import get_openai_client, call_openai_with_retry, clean_response_for_json_parsing
from .config import Config
from .pricing_tracker import PricingTracker

logger = logging.getLogger("ai_sast")

# Check if debug mode is enabled
OPENAI_DEBUG = os.getenv("OPENAI_DEBUG", "false").lower() in ("true", "1", "yes")
if OPENAI_DEBUG:
    logger.info("OpenAI debug mode is enabled")

# Create debug directory
debug_dir = Path("debug_logs")
debug_dir.mkdir(exist_ok=True, parents=True)

VULNERABILITY_SCHEMA = {
    "type": "object",
    "properties": {
        "vulnerability_type": {"type": "string"},
        "description": {"type": "string"},
        "location": {
            "type": "object",
            "properties": {
                "file": {"type": "string"},
                "line": {"type": ["integer", "string"]},
                "column": {"type": ["integer", "string", "null"]}
            },
            "required": ["file", "line"]
        },
        "severity": {"type": "string"},
        "recommendation": {"type": "string"}
    },
    "required": ["vulnerability_type", "description", "location", "severity", "recommendation"]
}

def get_vulnerability_prompt(file_path: Path, file_content: str, related_files: List[Path] = None) -> str:
    """
    Construct a prompt for vulnerability detection.
    
    Args:
        file_path: Path to the file
        file_content: Content of the file
        related_files: List of related files
        
    Returns:
        str: Prompt for OpenAI
    """
    related_files_info = ""
    if related_files and len(related_files) > 0:
        related_files_info = "This file has relationships with the following files:\n"
        for rf in related_files[:5]:
            related_files_info += f"- {rf.name}\n"
        if len(related_files) > 5:
            related_files_info += f"And {len(related_files) - 5} more files\n"
    
    prompt = {
        "ai_role": "cyber_security_expert",
        "request": "analyze this file for security vulnerabilities using static analysis. Pay special attention to:",
        "key_areas_to_check": [
            "Direct use of eval() or similar dangerous functions",
            "Unsanitized user input being used in DOM operations",
            "Direct HTML injection through innerHTML or similar",
            "Unsafe JavaScript execution",
            "Lack of input validation or sanitization",
            "Potential XSS vectors",
            "Code injection vulnerabilities",
            "DOM-based attacks"
        ],
        "frontend_vulnerabilities": [
            "Cross-Site Scripting (XSS)",
            "Cross-Site Request Forgery (CSRF)",
            "Insecure Authentication",
            "Insecure Data Storage",
            "Insecure API Endpoints",
            "Information Leakage",
            "Improper Access Control",
            "Insecure Dependencies",
            "Prototype Pollution",
            "Path Traversal",
            "Server-Side Request Forgery (SSRF)",
            "SQL Injection",
            "NoSQL Injection",
            "DOM-based vulnerabilities",
            "Sensitive Data Exposure",
            "Code Injection",
            "DOM XSS",
            "Client-Side Template Injection"
        ],
        "structure_of_vuln": {
            "vulnerability_type": "string - one of the vulnerability types listed above",
            "description": "string - detailed description of the vulnerability",
            "location": {
                "file": "string - file path relative to project root",
                "line": "integer - line number where vulnerability exists",
                "column": "integer - column number where vulnerability starts"
            },
            "severity": "string - must be one of: critical, high, medium, low",
            "recommendation": "string - specific remediation advice"
        },
        "example_vulnerabilities": [
            {
                "vulnerability_type": "Code Injection",
                "description": "Direct use of eval() with user input allows arbitrary code execution",
                "location": {
                    "file": "calculator.js",
                    "line": 5,
                    "column": 12
                },
                "severity": "critical",
                "recommendation": "Replace eval() with a safe expression evaluator or use a math library. If eval is absolutely necessary, implement strict input validation and sanitization."
            },
            {
                "vulnerability_type": "DOM XSS",
                "description": "Unsanitized user input is directly injected into the DOM using innerHTML",
                "location": {
                    "file": "comment.js",
                    "line": 4,
                    "column": 5
                },
                "severity": "high",
                "recommendation": "Use textContent instead of innerHTML for text-only content. If HTML is required, use a sanitization library like DOMPurify before injection."
            }
        ],
        "file_path": str(file_path),
        "related_files_info": related_files_info,
        "content_of_file": file_content,
        "output_format": {
            "format": "JSON array",
            "instructions": "Respond with a JSON array of vulnerability objects. If no vulnerabilities are found, return an empty array [].",
            "examples": [
                "[]",  # Empty array for no vulnerabilities
                '[{"vulnerability_type": "DOM XSS", "description": "...", "location": {"file": "comment.js", "line": 4, "column": 5}, "severity": "high", "recommendation": "..."}]'  # Array with vulnerabilities
            ],
            "important": "Return ONLY a JSON array with no other text, explanations, or markdown."
        }
    }
    
    return json.dumps(prompt, indent=2)


def extract_vulnerabilities(response_text: str, file_path: str = "") -> List[Dict[str, Any]]:
    """
    Extract and validate vulnerabilities from OpenAI response.
    Enhanced with better JSON handling.
    
    Args:
        response_text: Text response from OpenAI
        file_path: Path to the file being analyzed (for logging)
        
    Returns:
        List[Dict[str, Any]]: List of validated vulnerabilities
    """
    # Save raw response for debugging
    if OPENAI_DEBUG:
        debug_subdir = debug_dir / "raw_responses"
        debug_subdir.mkdir(exist_ok=True, parents=True)
        
        file_name = Path(file_path).name.replace(".", "_") if file_path else f"unknown_{int(time.time())}"
        raw_file = debug_subdir / f"{file_name}_raw.txt"
        
        with open(raw_file, 'w') as f:
            f.write(response_text)
        
        logger.debug(f"Saved raw response to {raw_file}")
    
    # Log response size
    logger.debug(f"Processing response of length {len(response_text)} chars")
    
    # Log the first and last part of the response for debugging
    preview_length = min(100, len(response_text))
    logger.debug(f"Response starts with: {response_text[:preview_length]}")
    if len(response_text) > preview_length * 2:
        logger.debug(f"Response ends with: {response_text[-preview_length:]}")
    
    # Clean and process the response
    cleaned_response = clean_response_for_json_parsing(response_text)
    
    # Save cleaned response for debugging
    if OPENAI_DEBUG:
        cleaned_file = debug_subdir / f"{file_name}_cleaned.txt"
        with open(cleaned_file, 'w') as f:
            f.write(cleaned_response)
        logger.debug(f"Saved cleaned response to {cleaned_file}")
    
    vulnerabilities = []
    
    try:
        # Parse the cleaned response as JSON
        try:
            data = json.loads(cleaned_response)
            
            if isinstance(data, list):
                logger.debug(f"Response is a JSON array with {len(data)} items")
                vulnerabilities = data
                
            elif isinstance(data, dict) and "vulnerability_type" in data:
                logger.debug("Response is a single vulnerability object")
                vulnerabilities = [data]
                
            elif isinstance(data, dict) and "vulnerabilities" in data:
                logger.debug(f"Response has 'vulnerabilities' key with {len(data['vulnerabilities'])} items")
                vulnerabilities = data["vulnerabilities"]
                
            else:
                logger.warning(f"JSON response has unexpected format: {type(data)}")
                
                # Try to find any vulnerability-like objects in the response
                if isinstance(data, dict):
                    for key, value in data.items():
                        if isinstance(value, list) and len(value) > 0:
                            # This might be the vulnerabilities list
                            potential_vulns = []
                            for item in value:
                                if isinstance(item, dict) and "vulnerability_type" in item:
                                    potential_vulns.append(item)
                            
                            if potential_vulns:
                                logger.debug(f"Found {len(potential_vulns)} potential vulnerabilities in key '{key}'")
                                vulnerabilities = potential_vulns
                                break
                
                if not vulnerabilities and OPENAI_DEBUG:
                    unexpected_file = debug_subdir / f"{file_name}_unexpected_format.json"
                    with open(unexpected_file, 'w') as f:
                        json.dump(data, f, indent=2)
                    logger.debug(f"Saved unexpected format to {unexpected_file}")
                
        except json.JSONDecodeError as e:
            logger.warning(f"Failed to parse response as JSON: {str(e)}")
            
            # Try to extract JSON objects using regex as a last resort
            import re
            # Pattern for finding JSON objects with vulnerability_type
            json_pattern = r'\{\s*"vulnerability_type"\s*:.*?\}\s*(?=\{|$)'
            matches = re.finditer(json_pattern, response_text, re.DOTALL)
            
            match_count = 0
            for match in matches:
                match_count += 1
                try:
                    vuln_text = match.group(0)
                    logger.debug(f"Found potential vulnerability JSON: {vuln_text[:50]}...")
                    vuln = json.loads(vuln_text)
                    vulnerabilities.append(vuln)
                except json.JSONDecodeError:
                    logger.warning(f"Failed to parse extracted JSON object")
                    continue
            
            logger.debug(f"Extracted {match_count} potential vulnerability objects with regex")
            
            # Try to extract JSON array if we found no individual objects
            if not vulnerabilities:
                # Try to find a JSON array in the response
                array_pattern = r'\[\s*\{.*?\}\s*(?:,\s*\{.*?\}\s*)*\]'
                array_match = re.search(array_pattern, response_text, re.DOTALL)
                if array_match:
                    try:
                        array_text = array_match.group(0)
                        logger.debug(f"Found potential JSON array: {array_text[:50]}...")
                        array_data = json.loads(array_text)
                        if isinstance(array_data, list):
                            vulnerabilities = array_data
                            logger.debug(f"Extracted {len(vulnerabilities)} vulnerabilities from JSON array")
                    except json.JSONDecodeError:
                        logger.warning("Failed to parse extracted JSON array")
            
            # If all parsing attempts failed, look for common security patterns as a fallback
            if not vulnerabilities:
                logger.warning("No JSON objects found in response, using fallback extraction")
                
                # Look for common vulnerability indicators
                common_indicators = {
                    "eval(": {
                        "vulnerability_type": "Code Injection",
                        "severity": "critical",
                        "recommendation": "Replace eval() with a safe expression evaluator or use a math library."
                    },
                    "innerHTML": {
                        "vulnerability_type": "DOM XSS",
                        "severity": "high",
                        "recommendation": "Use textContent instead of innerHTML for text-only content."
                    },
                    "document.write": {
                        "vulnerability_type": "DOM XSS", 
                        "severity": "high",
                        "recommendation": "Avoid using document.write as it can lead to XSS vulnerabilities."
                    },
                    "localStorage.setItem": {
                        "vulnerability_type": "Insecure Storage",
                        "severity": "medium",
                        "recommendation": "Don't store sensitive data in localStorage."
                    }
                }
                
                # Look for these indicators in the code content
                # We're looking directly in the file content instead of the response
                file_content = ""
                try:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        file_content = f.read()
                except Exception:
                    pass
                
                if file_content:
                    for indicator, vuln_info in common_indicators.items():
                        if indicator in file_content:
                            # Find the line number
                            lines = file_content.split('\n')
                            line_num = 0
                            for i, line in enumerate(lines):
                                if indicator in line:
                                    line_num = i + 1
                                    break
                            
                            logger.debug(f"Found indicator '{indicator}' in file at line {line_num}")
                            vulnerabilities.append({
                                "vulnerability_type": vuln_info["vulnerability_type"],
                                "description": f"Potential {vuln_info['vulnerability_type']} vulnerability detected: {indicator}",
                                "location": {
                                    "file": file_path,
                                    "line": line_num,
                                    "column": 0
                                },
                                "severity": vuln_info["severity"],
                                "recommendation": vuln_info["recommendation"]
                            })
                            
    except Exception as e:
        logger.error(f"Error extracting vulnerabilities: {str(e)}")
        logger.error(traceback.format_exc())
        return []
    
    # Validate each vulnerability against the schema
    validated_vulns = []
    for vuln in vulnerabilities:
        try:
            jsonschema.validate(instance=vuln, schema=VULNERABILITY_SCHEMA)
            validated_vulns.append(vuln)
        except jsonschema.exceptions.ValidationError as e:
            logger.warning(f"Invalid vulnerability format: {str(e)}")
            
            # Try to fix common validation issues
            try:
                # Make sure location has all required fields
                if "location" not in vuln:
                    vuln["location"] = {"file": file_path, "line": 0, "column": 0}
                elif "file" not in vuln["location"]:
                    vuln["location"]["file"] = file_path
                elif "line" not in vuln["location"]:
                    vuln["location"]["line"] = 0
                
                # Ensure number fields are actually numbers
                if isinstance(vuln["location"]["line"], str):
                    try:
                        vuln["location"]["line"] = int(vuln["location"]["line"])
                    except ValueError:
                        vuln["location"]["line"] = 0
                
                if "column" in vuln["location"] and isinstance(vuln["location"]["column"], str):
                    try:
                        vuln["location"]["column"] = int(vuln["location"]["column"])
                    except ValueError:
                        vuln["location"]["column"] = 0
                
                # Ensure severity is valid
                if "severity" not in vuln:
                    vuln["severity"] = "medium"
                elif vuln["severity"] not in ["critical", "high", "medium", "low"]:
                    # Map severity values to standard ones
                    severity_map = {
                        "severe": "critical",
                        "important": "high",
                        "moderate": "medium",
                        "minor": "low",
                        "info": "low"
                    }
                    vuln["severity"] = severity_map.get(vuln["severity"].lower(), "medium")
                
                # Ensure recommendation exists
                if "recommendation" not in vuln:
                    if "remedy" in vuln:
                        vuln["recommendation"] = vuln["remedy"]
                    else:
                        vuln["recommendation"] = "Review this code for security issues."
                
                # Validate again after fixes
                jsonschema.validate(instance=vuln, schema=VULNERABILITY_SCHEMA)
                validated_vulns.append(vuln)
                logger.debug("Fixed validation issues in vulnerability")
                
            except Exception:
                # If fixing failed, log the problem
                if OPENAI_DEBUG:
                    invalid_file = debug_dir / f"invalid_vuln_{len(validated_vulns)}.json"
                    with open(invalid_file, 'w') as f:
                        try:
                            json.dump(vuln, f, indent=2)
                        except:
                            f.write(str(vuln))
                    logger.debug(f"Saved invalid vulnerability to {invalid_file}")
    
    logger.info(f"Extracted {len(validated_vulns)} valid vulnerabilities from response")
    return validated_vulns


def detect_vulnerabilities(file_path: Path, file_content: str, related_files: List[Path], 
                          config: Config, project_name: str, pricing_tracker: Optional[PricingTracker] = None) -> List[Dict[str, Any]]:
    """
    Detect vulnerabilities in a file using OpenAI.
    Enhanced with robust error handling and debugging.
    
    Args:
        file_path: Path to the file
        file_content: Content of the file
        related_files: List of related files
        config: Configuration object
        project_name: Name of the project (for token tracking)
        pricing_tracker: Tracker for token usage and pricing
        
    Returns:
        List[Dict[str, Any]]: List of detected vulnerabilities
    """
    logger.info(f"Scanning file for vulnerabilities: {file_path}")
    start_time = time.time()
    
    try:
        # Create project-specific debug directory
        if OPENAI_DEBUG:
            project_debug_dir = debug_dir / project_name
            project_debug_dir.mkdir(exist_ok=True, parents=True)
            logger.debug(f"Created debug directory for project: {project_debug_dir}")
            
            # Save file content for debugging
            file_debug_dir = project_debug_dir / "files"
            file_debug_dir.mkdir(exist_ok=True, parents=True)
            debug_file_path = file_debug_dir / f"{file_path.name}.txt"
            with open(debug_file_path, 'w', encoding='utf-8', errors='ignore') as f:
                f.write(file_content)
            logger.debug(f"Saved file content to {debug_file_path}")
        
        prompt = get_vulnerability_prompt(file_path, file_content, related_files)
        
        # Save prompt to file in debug mode
        if OPENAI_DEBUG:
            prompt_debug_dir = project_debug_dir / "prompts"
            prompt_debug_dir.mkdir(exist_ok=True, parents=True)
            
            file_name = file_path.name.replace(".", "_")
            prompt_file = prompt_debug_dir / f"{file_name}_prompt.json"
            
            with open(prompt_file, 'w') as f:
                f.write(prompt)
            
            logger.debug(f"Saved prompt for {file_path} to {prompt_file}")
        
        # Get the OpenAI client
        try:
            client = get_openai_client()
        except Exception as e:
            logger.error(f"Failed to get OpenAI client: {str(e)}")
            if OPENAI_DEBUG:
                error_file = project_debug_dir / f"client_error_{file_path.name}.txt"
                with open(error_file, 'w') as f:
                    f.write(f"Error getting OpenAI client: {str(e)}\n")
                    f.write(traceback.format_exc())
            
            # Return a placeholder vulnerability to indicate the error
            return [{
                "vulnerability_type": "API Connection Error",
                "description": f"Failed to connect to OpenAI API: {str(e)}",
                "location": {
                    "file": str(file_path),
                    "line": 0,
                    "column": 0
                },
                "severity": "info",
                "recommendation": "Check your API key and network connection."
            }]
            
        # Log model being used
        logger.info(f"Using model: {config.model} for scanning {file_path}")
        
        try:
            # Create system message with strong JSON enforcement
            system_message = (
                "You are a cybersecurity expert specializing in finding security vulnerabilities in code. "
                "Your task is to analyze code for security issues and return vulnerabilities in a JSON array format. "
                "IMPORTANT: Respond with ONLY a valid JSON array of vulnerability objects. No other text before or after. "
                "If no vulnerabilities are found, return an empty array: []"
            )
            
            # Make the API call with retry logic
            response = call_openai_with_retry(
                client=client,
                model=config.model,
                messages=[
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.0,  # Use 0 temperature for consistent output format
                max_tokens=config.max_tokens,
                max_retries=config.max_retries,
                retry_delay=config.retry_delay
            )
            
            elapsed_time = time.time() - start_time
            logger.info(f"OpenAI call succeeded in {elapsed_time:.2f}s")
            
            # Track token usage
            if pricing_tracker and hasattr(response, 'usage'):
                input_tokens = response.usage.prompt_tokens
                output_tokens = response.usage.completion_tokens
                total_tokens = response.usage.total_tokens
                
                logger.info(f"Token usage: Prompt={input_tokens}, Completion={output_tokens}, Total={total_tokens}")
                
                pricing_tracker.track_usage(
                    project_name=project_name,
                    input_tokens=input_tokens,
                    output_tokens=output_tokens
                )
            
            response_text = response.choices[0].message.content
            
            # Save response to file in debug mode
            if OPENAI_DEBUG:
                response_debug_dir = project_debug_dir / "responses"
                response_debug_dir.mkdir(exist_ok=True, parents=True)
                
                response_file = response_debug_dir / f"{file_name}_response.json"
                with open(response_file, 'w') as f:
                    f.write(response_text)
                
                logger.debug(f"Saved response for {file_path} to {response_file}")
            
            vulnerabilities = extract_vulnerabilities(response_text, str(file_path))
            
            # Ensure each vulnerability has the correct file path
            for vuln in vulnerabilities:
                vuln["location"]["file"] = str(file_path)
            
            logger.info(f"Found {len(vulnerabilities)} vulnerabilities in {file_path}")
            return vulnerabilities
            
        except Exception as e:
            logger.error(f"Error calling OpenAI API for {file_path}: {str(e)}")
            logger.error(traceback.format_exc())
            
            # Save error information to debug file
            if OPENAI_DEBUG:
                error_debug_dir = project_debug_dir / "errors"
                error_debug_dir.mkdir(exist_ok=True, parents=True)
                
                error_file = error_debug_dir / f"{file_name}_error.txt"
                with open(error_file, 'w') as f:
                    f.write(f"Error scanning {file_path}:\n{str(e)}\n\n")
                    f.write(traceback.format_exc())
                
                # Also dump environment info
                env_file = error_debug_dir / f"{file_name}_environment.txt"
                with open(env_file, 'w') as f:
                    # Mask sensitive values
                    safe_env = {}
                    for key, value in os.environ.items():
                        if "api" in key.lower() or "key" in key.lower() or "token" in key.lower() or "secret" in key.lower():
                            if value and len(value) > 4:
                                safe_env[key] = f"{value[:4]}...{value[-4:]}" 
                            else:
                                safe_env[key] = "[masked]"
                        else:
                            safe_env[key] = value
                    
                    f.write(json.dumps(safe_env, indent=2))
                
                logger.debug(f"Saved error info for {file_path} to {error_file}")
            
            # Return a placeholder vulnerability to indicate the error
            return [{
                "vulnerability_type": "API Error",
                "description": f"Error during vulnerability scanning: {str(e)}",
                "location": {
                    "file": str(file_path),
                    "line": 0,
                    "column": 0
                },
                "severity": "info",
                "recommendation": "Check the logs for more details."
            }]
    
    except Exception as e:
        logger.error(f"Error detecting vulnerabilities in {file_path}: {str(e)}")
        logger.error(traceback.format_exc())
        return [{
            "vulnerability_type": "Processing Error",
            "description": f"Error processing file: {str(e)}",
            "location": {
                "file": str(file_path),
                "line": 0,
                "column": 0
            },
            "severity": "info",
            "recommendation": "Check the logs for more details."
        }]


def write_vulnerability_report(vulnerabilities: List[Dict[str, Any]], output_dir: Path) -> None:
    """
    Write vulnerability report to JSON files.
    
    Args:
        vulnerabilities: List of vulnerabilities
        output_dir: Output directory
    """
    try:
        # Group vulnerabilities by type
        vuln_by_type = {}
        for vuln in vulnerabilities:
            vuln_type = vuln.get("vulnerability_type", "Unknown")
            sanitized_name = vuln_type.lower().replace(" ", "_").replace("(", "").replace(")", "").replace("-", "_")
            
            if sanitized_name not in vuln_by_type:
                vuln_by_type[sanitized_name] = []
            
            vuln_by_type[sanitized_name].append(vuln)
        
        # Write individual files for each type
        for vuln_type, vulns in vuln_by_type.items():
            file_path = output_dir / f"{vuln_type}.json"
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(vulns, f, indent=2)
            
            logger.info(f"Wrote {len(vulns)} vulnerabilities to {file_path}")
        
        # Write a consolidated file with all vulnerabilities
        all_vulns_path = output_dir / "all_vulnerabilities.json"
        with open(all_vulns_path, 'w', encoding='utf-8') as f:
            json.dump(vulnerabilities, f, indent=2)
        
        logger.info(f"Wrote {len(vulnerabilities)} total vulnerabilities to {all_vulns_path}")
        
    except Exception as e:
        logger.error(f"Error writing vulnerability report: {str(e)}")
        logger.error(traceback.format_exc())


def scan_files_for_vulnerabilities(
    files: List[Path], 
    file_relationships: Dict[Path, List[Path]], 
    config: Config, 
    progress: Progress, 
    task_id,
    pricing_tracker: Optional[PricingTracker] = None
) -> Dict[Path, List[Dict[str, Any]]]:
    """
    Scan files for vulnerabilities.
    Enhanced with better error handling and debugging.
    
    Args:
        files: List of files to scan
        file_relationships: Dictionary mapping files to their dependencies
        config: Configuration object
        progress: Progress object for updating the progress bar
        task_id: Task ID for the progress bar
        pricing_tracker: Tracker for token usage and pricing
        
    Returns:
        Dict[Path, List[Dict[str, Any]]]: Dictionary mapping files to their vulnerabilities
    """
    all_vulnerabilities = {}
    file_count = len(files)
    
    # Extract project name from the config or first file path
    # Assumes all files are from the same project
    if hasattr(config, 'project_name') and config.project_name:
        project_name = config.project_name
    elif len(files) > 0 and len(files[0].parts) >= 2:
        project_name = files[0].parts[-2]
    else:
        project_name = "unknown_project"
    
    logger.info(f"Starting vulnerability scan for project: {project_name}")
    
    # Create a debug log directory for this scan
    scan_debug_dir = debug_dir / project_name / f"scan_{time.strftime('%Y%m%d_%H%M%S')}"
    if OPENAI_DEBUG:
        scan_debug_dir.mkdir(exist_ok=True, parents=True)
        logger.debug(f"Created scan debug directory: {scan_debug_dir}")
    
    # Run CodeQL analysis if enabled
    if config.enable_codeql:
        progress.update(task_id, description="[cyan]Running CodeQL analysis...", completed=10)
        
        try:
            # Try to import the CodeQL scanner module
            try:
                from .codeql_scanner import run_codeql_scan
                codeql_available = True
            except (ImportError, ModuleNotFoundError):
                logger.warning("CodeQL scanner module not available, skipping CodeQL analysis")
                codeql_available = False
            
            if codeql_available:
                # Create paths for CodeQL database and results
                logs_folder_name = config.get_logs_folder_name()
                output_subdir = Path(config.output_dir) / logs_folder_name
                
                # Run the analysis and get results
                src_dir = Path(config.src_dir)
                codeql_findings = run_codeql_scan(src_dir, output_subdir)
                
                # Process findings into our format
                for finding in codeql_findings:
                    # Get the file path from the finding
                    file_path_str = finding.get("location", {}).get("file", "")
                    
                    # Skip if no file path
                    if not file_path_str:
                        continue
                    
                    # Convert to a relative path if it's absolute
                    if os.path.isabs(file_path_str):
                        try:
                            # Try to make it relative to the source directory
                            rel_path = os.path.relpath(file_path_str, config.src_dir)
                            file_path = src_dir / rel_path
                        except ValueError:
                            # If that fails, use the path as is
                            file_path = Path(file_path_str)
                    else:
                        # If it's already relative, make it relative to src_dir
                        file_path = src_dir / file_path_str
                    
                    # Add to our vulnerability dictionary
                    if file_path not in all_vulnerabilities:
                        all_vulnerabilities[file_path] = []
                    
                    # Add source information to the finding
                    finding["source"] = "CodeQL"
                    
                    all_vulnerabilities[file_path].append(finding)
                
                # Write CodeQL findings to a separate file
                codeql_output_path = output_subdir / "codeql_vulnerabilities.json"
                with open(codeql_output_path, 'w', encoding='utf-8') as f:
                    json.dump(codeql_findings, f, indent=2)
                
                progress.update(task_id, description=f"[green]CodeQL found {len(codeql_findings)} issues", completed=30)
                logger.info(f"CodeQL analysis found {len(codeql_findings)} potential vulnerabilities")
        except Exception as e:
            logger.error(f"Error running CodeQL analysis: {str(e)}")
            logger.exception("CodeQL analysis failed")
            
            if OPENAI_DEBUG:
                codeql_error_file = scan_debug_dir / "codeql_error.txt"
                with open(codeql_error_file, 'w') as f:
                    f.write(f"Error running CodeQL analysis: {str(e)}\n\n")
                    f.write(traceback.format_exc())
    else:
        logger.info("CodeQL analysis is disabled, skipping")

    # Use a batch size parameter from config with a reasonable default
    batch_size = min(config.batch_size, 10)
    
    # Split files into batches for processing
    file_batches = [files[i:i+batch_size] for i in range(0, len(files), batch_size)]
    logger.info(f"Processing {len(files)} files in {len(file_batches)} batches of up to {batch_size} files each")
    
    completed = 0
    batch_number = 0
    
    for batch in file_batches:
        batch_number += 1
        batch_results = {}
        
        logger.info(f"Processing batch {batch_number}/{len(file_batches)} with {len(batch)} files")
        
        # Update progress
        progress.update(task_id, description=f"[cyan]Processing batch {batch_number}/{len(file_batches)}...", 
                       completed=30 + (batch_number * 70 / len(file_batches)))
        
        with ThreadPoolExecutor(max_workers=min(batch_size, 5)) as executor:
            future_to_file = {}
            
            for file in batch:
                try:
                    # Read file content
                    try:
                        with open(file, 'r', encoding='utf-8', errors='ignore') as f:
                            content = f.read()
                    except Exception as e:
                        logger.error(f"Error reading {file}: {str(e)}")
                        batch_results[file] = [{
                            "vulnerability_type": "File Read Error", 
                            "description": f"Could not read file: {str(e)}",
                            "location": {"file": str(file), "line": 0, "column": 0},
                            "severity": "info",
                            "recommendation": "Check file permissions and encoding."
                        }]
                        continue
                    
                    # Get related files
                    related_files = file_relationships.get(file, [])
                    
                    # Submit the detection task to the executor
                    future = executor.submit(
                        detect_vulnerabilities, 
                        file, 
                        content, 
                        related_files, 
                        config,
                        project_name,
                        pricing_tracker
                    )
                    future_to_file[future] = file
                except Exception as e:
                    logger.error(f"Error processing {file}: {str(e)}")
                    batch_results[file] = [{
                        "vulnerability_type": "Processing Error", 
                        "description": f"Error preparing file for scanning: {str(e)}",
                        "location": {"file": str(file), "line": 0, "column": 0},
                        "severity": "info",
                        "recommendation": "Check the logs for more details."
                    }]
            
            # Process the results as they complete
            for future in future_to_file:
                file = future_to_file[future]
                try:
                    vulnerabilities = future.result()
                    batch_results[file] = vulnerabilities
                    
                    completed += 1
                    progress.update(task_id, description=f"[cyan]Scanning files ({completed}/{file_count})...", 
                                   completed=30 + (completed * 70 / file_count))
                    
                except Exception as e:
                    logger.error(f"Error processing {file}: {str(e)}")
                    logger.error(traceback.format_exc())
                    batch_results[file] = [{
                        "vulnerability_type": "Scan Error", 
                        "description": f"Error during vulnerability scanning: {str(e)}",
                        "location": {"file": str(file), "line": 0, "column": 0},
                        "severity": "info",
                        "recommendation": "Check the logs for more details."
                    }]
        
        # Update the global results with this batch
        all_vulnerabilities.update(batch_results)
        
        # Add a small delay between batches to avoid rate limiting
        if len(file_batches) > 1 and batch_number < len(file_batches):
            logger.debug(f"Waiting 1 second before processing next batch")
            time.sleep(1)
    
    # Create output directory for vulnerability reports
    logs_folder_name = config.get_logs_folder_name()
    output_subdir = Path(config.output_dir) / logs_folder_name
    output_subdir.mkdir(exist_ok=True, parents=True)
    
    # Collect all vulnerabilities into a single list
    all_vulns_list = []
    for file_vulns in all_vulnerabilities.values():
        all_vulns_list.extend(file_vulns)
    
    # Write the vulnerability reports
    if all_vulns_list:
        write_vulnerability_report(all_vulns_list, output_subdir)
    
    # Log vulnerability summary
    logger.info(f"Vulnerability scan complete. Found {len(all_vulns_list)} vulnerabilities in {len(files)} files.")
    
    # Count vulnerabilities by severity
    severity_counts = {"critical": 0, "high": 0, "medium": 0, "low": 0, "info": 0}
    for vuln in all_vulns_list:
        severity = vuln.get("severity", "").lower()
        if severity in severity_counts:
            severity_counts[severity] += 1
    
    # Log severity summary
    logger.info(f"Vulnerability severity breakdown: {severity_counts}")
    
    return all_vulnerabilities