"""
Vulnerability detector module for AI_SAST.

Uses OpenAI to detect vulnerabilities in code files.
"""

import os
import json
import logging
import time
from pathlib import Path
from typing import Dict, List, Any, Tuple
from concurrent.futures import ThreadPoolExecutor
from rich.progress import Progress
import jsonschema

from .openai_client import get_openai_client, call_openai_with_retry
from .config import Config

logger = logging.getLogger("ai_sast")

# Vulnerability schema for validation
VULNERABILITY_SCHEMA = {
    "type": "object",
    "properties": {
        "vulnerability_type": {"type": "string"},
        "description": {"type": "string"},
        "location": {
            "type": "object",
            "properties": {
                "file": {"type": "string"},
                "line": {"type": ["integer", "string"]},
                "column": {"type": ["integer", "string", "null"]}
            },
            "required": ["file", "line"]
        },
        "severity": {"type": "string"},
        "recommendation": {"type": "string"}
    },
    "required": ["vulnerability_type", "description", "location", "severity", "recommendation"]
}

# Example vulnerabilities to guide the AI
EXAMPLE_VULNERABILITIES = [
    {
        "vulnerability_type": "Cross-Site Scripting (XSS)",
        "description": "Unsanitized user input is directly inserted into the HTML, allowing attackers to inject malicious scripts.",
        "location": {
            "file": "./components/UserProfile.jsx",
            "line": 42,
            "column": 15
        },
        "severity": "high",
        "recommendation": "Use React's built-in XSS protection by using curly braces for dynamic content. For HTML attributes, consider using a sanitization library like DOMPurify."
    },
    {
        "vulnerability_type": "Insecure Authentication",
        "description": "Credentials are stored in local storage which is vulnerable to XSS attacks.",
        "location": {
            "file": "./services/auth.js",
            "line": 78,
            "column": 3
        },
        "severity": "critical",
        "recommendation": "Use HttpOnly cookies for sensitive authentication tokens instead of local storage. Consider using a secure authentication library or service."
    }
]


def get_vulnerability_prompt(file_path: Path, file_content: str, related_files: List[Path] = None) -> str:
    """
    Construct a prompt for vulnerability detection.
    
    Args:
        file_path: Path to the file
        file_content: Content of the file
        related_files: List of related files
        
    Returns:
        str: Prompt for OpenAI
    """
    # Prepare related files information
    related_files_info = ""
    if related_files and len(related_files) > 0:
        related_files_info = "This file has relationships with the following files:\n"
        for rf in related_files[:5]:  # Limit to 5 related files to save tokens
            related_files_info += f"- {rf.name}\n"
        if len(related_files) > 5:
            related_files_info += f"And {len(related_files) - 5} more files\n"
    
    # Construct the prompt
    prompt = {
        "ai_role": "cyber_security_expert",
        "request": "analyze this file for security vulnerabilities using static analysis",
        "frontend_vulnerabilities": [
            "Cross-Site Scripting (XSS)",
            "Cross-Site Request Forgery (CSRF)",
            "Insecure Authentication",
            "Insecure Data Storage",
            "Insecure API Endpoints",
            "Information Leakage",
            "Improper Access Control",
            "Insecure Dependencies",
            "Prototype Pollution",
            "Path Traversal",
            "Server-Side Request Forgery (SSRF)",
            "SQL Injection",
            "NoSQL Injection",
            "DOM-based vulnerabilities",
            "Sensitive Data Exposure"
        ],
        "structure_of_vuln": {
            "vulnerability_type": "string - one of the vulnerability types listed above",
            "description": "string - detailed description of the vulnerability",
            "location": {
                "file": "string - file path relative to project root",
                "line": "integer - line number where vulnerability exists",
                "column": "integer - column number where vulnerability starts"
            },
            "severity": "string - must be one of: critical, high, medium, low",
            "recommendation": "string - specific remediation advice"
        },
        "example_vulnerabilities": EXAMPLE_VULNERABILITIES,
        "file_path": str(file_path),
        "related_files_info": related_files_info,
        "content_of_file": file_content
    }
    
    # Convert to a well-formatted string
    return json.dumps(prompt, indent=2)


def extract_vulnerabilities(response_text: str) -> List[Dict[str, Any]]:
    """
    Extract and validate vulnerabilities from OpenAI response.
    
    Args:
        response_text: Text response from OpenAI
        
    Returns:
        List[Dict[str, Any]]: List of validated vulnerabilities
    """
    vulnerabilities = []
    
    try:
        # Try to parse the entire response as JSON
        data = json.loads(response_text)
        
        # Check if it's a list of vulnerabilities
        if isinstance(data, list):
            vulnerabilities = data
        # Check if it's a single vulnerability object
        elif isinstance(data, dict) and "vulnerability_type" in data:
            vulnerabilities = [data]
        # Check if it has a 'vulnerabilities' field
        elif isinstance(data, dict) and "vulnerabilities" in data:
            vulnerabilities = data["vulnerabilities"]
    except json.JSONDecodeError:
        # If not valid JSON, try to extract JSON objects
        import re
        json_pattern = r'\{\s*"vulnerability_type"\s*:.*?\}\s*(?=\{|$)'
        matches = re.finditer(json_pattern, response_text, re.DOTALL)
        
        for match in matches:
            try:
                vuln = json.loads(match.group(0))
                vulnerabilities.append(vuln)
            except json.JSONDecodeError:
                continue
    
    # Validate each vulnerability
    validated_vulns = []
    for vuln in vulnerabilities:
        try:
            jsonschema.validate(instance=vuln, schema=VULNERABILITY_SCHEMA)
            validated_vulns.append(vuln)
        except jsonschema.exceptions.ValidationError as e:
            logger.warning(f"Invalid vulnerability format: {str(e)}")
    
    return validated_vulns


def detect_vulnerabilities(file_path: Path, file_content: str, related_files: List[Path], config: Config) -> List[Dict[str, Any]]:
    """
    Detect vulnerabilities in a file using OpenAI.
    
    Args:
        file_path: Path to the file
        file_content: Content of the file
        related_files: List of related files
        config: Configuration object
        
    Returns:
        List[Dict[str, Any]]: List of detected vulnerabilities
    """
    try:
        # Get the prompt for this file
        prompt = get_vulnerability_prompt(file_path, file_content, related_files)
        
        # Get OpenAI client
        client = get_openai_client()
        
        # Call OpenAI with retry logic
        response = call_openai_with_retry(
            client=client,
            model=config.model,
            messages=[
                {"role": "system", "content": "You are a cybersecurity expert specializing in finding security vulnerabilities in code. Respond with JSON only."},
                {"role": "user", "content": prompt}
            ],
            temperature=config.temperature,
            max_tokens=config.max_tokens,
            max_retries=config.max_retries,
            retry_delay=config.retry_delay
        )
        
        # Extract vulnerabilities from the response
        response_text = response.choices[0].message.content
        vulnerabilities = extract_vulnerabilities(response_text)
        
        # Ensure file path is correctly set
        for vuln in vulnerabilities:
            vuln["location"]["file"] = str(file_path)
        
        return vulnerabilities
    
    except Exception as e:
        logger.error(f"Error detecting vulnerabilities in {file_path}: {str(e)}")
        return []


def write_vulnerability_report(vulnerabilities: List[Dict[str, Any]], output_dir: Path) -> None:
    """
    Write vulnerability report to JSON files.
    
    Args:
        vulnerabilities: List of vulnerabilities
        output_dir: Output directory
    """
    # Group vulnerabilities by type
    vuln_by_type = {}
    for vuln in vulnerabilities:
        vuln_type = vuln["vulnerability_type"]
        sanitized_name = vuln_type.lower().replace(" ", "_").replace("(", "").replace(")", "").replace("-", "_")
        
        if sanitized_name not in vuln_by_type:
            vuln_by_type[sanitized_name] = []
        
        vuln_by_type[sanitized_name].append(vuln)
    
    # Write each vulnerability type to a separate file
    for vuln_type, vulns in vuln_by_type.items():
        file_path = output_dir / f"{vuln_type}.json"
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(vulns, f, indent=2)


def scan_files_for_vulnerabilities(
    files: List[Path], 
    file_relationships: Dict[Path, List[Path]], 
    config: Config, 
    progress: Progress, 
    task_id
) -> Dict[Path, List[Dict[str, Any]]]:
    """
    Scan files for vulnerabilities.
    
    Args:
        files: List of files to scan
        file_relationships: Dictionary mapping files to their dependencies
        config: Configuration object
        progress: Progress object for updating the progress bar
        task_id: Task ID for the progress bar
        
    Returns:
        Dict[Path, List[Dict[str, Any]]]: Dictionary mapping files to their vulnerabilities
    """
    all_vulnerabilities = {}
    file_count = len(files)
    
    # Process files in batches to avoid overloading the API
    batch_size = min(config.batch_size, 10)  # Maximum 10 files per batch
    
    # Group files into batches
    file_batches = [files[i:i+batch_size] for i in range(0, len(files), batch_size)]
    
    completed = 0
    
    for batch in file_batches:
        # Process this batch of files
        batch_results = {}
        
        with ThreadPoolExecutor(max_workers=min(batch_size, 5)) as executor:
            future_to_file = {}
            
            for file in batch:
                try:
                    # Read the file content
                    with open(file, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                    
                    # Get related files
                    related_files = file_relationships.get(file, [])
                    
                    # Submit the task
                    future = executor.submit(
                        detect_vulnerabilities, 
                        file, 
                        content, 
                        related_files, 
                        config
                    )
                    future_to_file[future] = file
                except Exception as e:
                    logger.error(f"Error reading {file}: {str(e)}")
                    batch_results[file] = []
            
            # Process completed tasks
            for future in future_to_file:
                file = future_to_file[future]
                try:
                    vulnerabilities = future.result()
                    batch_results[file] = vulnerabilities
                    
                    # Update progress
                    completed += 1
                    progress.update(task_id, description=f"[cyan]Scanning files ({completed}/{file_count})...", completed=completed)
                    
                except Exception as e:
                    logger.error(f"Error processing {file}: {str(e)}")
                    batch_results[file] = []
        
        # Update all_vulnerabilities with batch results
        all_vulnerabilities.update(batch_results)
        
        # Small delay between batches to avoid rate limiting
        if len(file_batches) > 1:
            time.sleep(1)
    
    # Write vulnerability reports
    project_name = Path(config.src_dir).name
    output_subdir = Path(config.output_dir) / f"{project_name}_logs"
    output_subdir.mkdir(exist_ok=True, parents=True)
    
    # Flatten the vulnerabilities list
    all_vulns_list = []
    for file_vulns in all_vulnerabilities.values():
        all_vulns_list.extend(file_vulns)
    
    # Write the reports
    if all_vulns_list:
        write_vulnerability_report(all_vulns_list, output_subdir)
    
    return all_vulnerabilities